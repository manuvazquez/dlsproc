# AUTOGENERATED! DO NOT EDIT! File to edit: 70_extend.ipynb (unless otherwise specified).

__all__ = ['parquet_with_zip']

# Cell
import pathlib

import pandas as pd
import yaml

import dlsproc.structure
import dlsproc.bundle
import dlsproc.hier
import dlsproc.io
import dlsproc.assemble

# Cell
def parquet_with_zip(history_file: str | pathlib.Path, zip_file: str | pathlib.Path, output_file: str | pathlib.Path | None = None) -> None | pd.DataFrame:

    # in case `str`s were passed
    history_file = pathlib.Path(history_file)
    zip_file = pathlib.Path(zip_file)

    assert history_file.exists(), f"can't find {history_file}"
    assert zip_file.exists(), f"can't find {zip_file}"

    # historical data
    history_df = pd.read_parquet(history_file)

    # new data is parsed into a `pd.DataFrame`...
    new_df = dlsproc.bundle.read_zip(zip_file, concatenate=True)

    # ...whose columns are a *multiindex*
    new_df = dlsproc.hier.flat_df_to_multiindexed_df(new_df)

    # we also need the information about which entries are flagged as deleted
    deleted_series = dlsproc.bundle.read_deleted_zip(zip_file)

    # the two dataframes are stacked together
    concatenated_df = dlsproc.assemble.stack(history_df, new_df)

    # for the sake of flagging deleted entries, the new (concatenated) dataframe and the *deleted series* are indexed the same
    reindexed_concatenated_df = concatenated_df.reset_index().set_index(['id'])
    reindexed_deleted_series = deleted_series.droplevel(0)

    # duplicates are dropped from the *deleted series*
    deduplicated_reindexed_deleted_series = reindexed_deleted_series.groupby(reindexed_deleted_series.index, group_keys=False).nsmallest(1)

    # the `deleted_on` column in the dataframe is filled in whenever appropriate
    reindexed_concatenated_df['deleted_on'] = reindexed_concatenated_df['deleted_on'].fillna(deduplicated_reindexed_deleted_series)

    # the orginal index is set back in place
    res = reindexed_concatenated_df.set_index(['file name', 'entry'])

    if output_file:

        # so that we can exploit pathlib's API...
        output_file = pathlib.Path(output_file)

        # ...for checking stuff
        assert output_file.suffix == '.parquet', f'output file should end in ".parquet"'

        parquet_df = dlsproc.assemble.parquet_amenable(res)

        parquet_df.to_parquet(output_file)

    else:

        return res